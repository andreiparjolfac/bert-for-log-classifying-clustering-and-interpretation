{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\APARJOL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import transformers \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"data/dataprocesed.csv\")\n",
    "labels = df[\"Issue Type\"].unique()\n",
    "labels = {labels[i]:i for i in range(len(labels))}\n",
    "df['Issue Type']=df['Issue Type'].apply(lambda x:labels[x])\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!~\n",
      "14751 total logfiles!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "MAX_LEN = 128\n",
    "input_ids = []\n",
    "train_labels = df[\"Issue Type\"].to_numpy().astype(int)\n",
    "chunk_labels = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in range(1,5):\n",
    "        col=\"JuicePath.\"+str(j)\n",
    "        if isinstance(df[col][i],str):\n",
    "            encoded_sent = tokenizer.encode(df[col][i],add_special_tokens=True)\n",
    "            label = train_labels[i]\n",
    "            if len(encoded_sent)>MAX_LEN:\n",
    "                encoded_sent=encoded_sent[1:-1]\n",
    "                chunk_len = MAX_LEN-2\n",
    "                for k in range(0,len(encoded_sent),chunk_len):\n",
    "                    tokens = encoded_sent[k:k+chunk_len]\n",
    "                    chunk = [tokenizer.cls_token_id] + tokens + [tokenizer.sep_token_id]\n",
    "                    input_ids.append(chunk)\n",
    "                    chunk_labels.append(label)\n",
    "            else:\n",
    "                input_ids.append(encoded_sent)\n",
    "                chunk_labels.append(label)\n",
    "print(\"DONE!~\")\n",
    "print(f\"{len(input_ids)} total logfiles!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(input_ids)==len(chunk_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14751, 128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_ids = pad_sequences(input_ids,\n",
    "                          maxlen=MAX_LEN,\n",
    "                          dtype=\"long\",\n",
    "                          value=tokenizer.pad_token_id,\n",
    "                          truncating=\"post\",\n",
    "                          padding=\"post\")\n",
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "for sent in input_ids:\n",
    "    att_mask = [int(x>0) for x in sent]\n",
    "    attention_masks.append(att_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs ,validation_inputs ,train_labels  , validation_labels = train_test_split(input_ids,chunk_labels,random_state=999,test_size=0.05)\n",
    "train_masks , validation_masks , _ , _ = train_test_split(attention_masks,attention_masks,random_state=999,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs=torch.tensor(train_inputs)\n",
    "validation_inputs=torch.tensor(validation_inputs)\n",
    "train_labels=torch.tensor(train_labels)\n",
    "validation_labels=torch.tensor(validation_labels)\n",
    "train_masks=torch.tensor(train_masks)\n",
    "validation_masks=torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=batch_size)\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\n",
    "                                   sampler=validation_sampler,\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification,AdamW,BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(labels),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\APARJOL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-8)\n",
    "from transformers import get_linear_schedule_with_warmup \n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds,labels):\n",
    "    pred_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat==labels_flat)/len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded=int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:04\n",
      "Batch 20 of 438 . Elapsed : 0:00:07\n",
      "Batch 30 of 438 . Elapsed : 0:00:10\n",
      "Batch 40 of 438 . Elapsed : 0:00:13\n",
      "Batch 50 of 438 . Elapsed : 0:00:16\n",
      "Batch 60 of 438 . Elapsed : 0:00:18\n",
      "Batch 70 of 438 . Elapsed : 0:00:21\n",
      "Batch 80 of 438 . Elapsed : 0:00:24\n",
      "Batch 90 of 438 . Elapsed : 0:00:27\n",
      "Batch 100 of 438 . Elapsed : 0:00:30\n",
      "Batch 110 of 438 . Elapsed : 0:00:32\n",
      "Batch 120 of 438 . Elapsed : 0:00:35\n",
      "Batch 130 of 438 . Elapsed : 0:00:38\n",
      "Batch 140 of 438 . Elapsed : 0:00:41\n",
      "Batch 150 of 438 . Elapsed : 0:00:44\n",
      "Batch 160 of 438 . Elapsed : 0:00:46\n",
      "Batch 170 of 438 . Elapsed : 0:00:49\n",
      "Batch 180 of 438 . Elapsed : 0:00:52\n",
      "Batch 190 of 438 . Elapsed : 0:00:55\n",
      "Batch 200 of 438 . Elapsed : 0:00:58\n",
      "Batch 210 of 438 . Elapsed : 0:01:01\n",
      "Batch 220 of 438 . Elapsed : 0:01:03\n",
      "Batch 230 of 438 . Elapsed : 0:01:06\n",
      "Batch 240 of 438 . Elapsed : 0:01:09\n",
      "Batch 250 of 438 . Elapsed : 0:01:12\n",
      "Batch 260 of 438 . Elapsed : 0:01:15\n",
      "Batch 270 of 438 . Elapsed : 0:01:18\n",
      "Batch 280 of 438 . Elapsed : 0:01:20\n",
      "Batch 290 of 438 . Elapsed : 0:01:23\n",
      "Batch 300 of 438 . Elapsed : 0:01:26\n",
      "Batch 310 of 438 . Elapsed : 0:01:29\n",
      "Batch 320 of 438 . Elapsed : 0:01:32\n",
      "Batch 330 of 438 . Elapsed : 0:01:35\n",
      "Batch 340 of 438 . Elapsed : 0:01:37\n",
      "Batch 350 of 438 . Elapsed : 0:01:40\n",
      "Batch 360 of 438 . Elapsed : 0:01:43\n",
      "Batch 370 of 438 . Elapsed : 0:01:46\n",
      "Batch 380 of 438 . Elapsed : 0:01:49\n",
      "Batch 390 of 438 . Elapsed : 0:01:52\n",
      "Batch 400 of 438 . Elapsed : 0:01:55\n",
      "Batch 410 of 438 . Elapsed : 0:01:57\n",
      "Batch 420 of 438 . Elapsed : 0:02:00\n",
      "Batch 430 of 438 . Elapsed : 0:02:03\n",
      "\n",
      " Average training Loss : 0.63\n",
      "Training epoch took : 0:02:05\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.90\n",
      "Validation took : 0:00:38\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:09\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:17\n",
      "Batch 70 of 438 . Elapsed : 0:00:20\n",
      "Batch 80 of 438 . Elapsed : 0:00:22\n",
      "Batch 90 of 438 . Elapsed : 0:00:25\n",
      "Batch 100 of 438 . Elapsed : 0:00:28\n",
      "Batch 110 of 438 . Elapsed : 0:00:30\n",
      "Batch 120 of 438 . Elapsed : 0:00:33\n",
      "Batch 130 of 438 . Elapsed : 0:00:36\n",
      "Batch 140 of 438 . Elapsed : 0:00:38\n",
      "Batch 150 of 438 . Elapsed : 0:00:41\n",
      "Batch 160 of 438 . Elapsed : 0:00:44\n",
      "Batch 170 of 438 . Elapsed : 0:00:46\n",
      "Batch 180 of 438 . Elapsed : 0:00:49\n",
      "Batch 190 of 438 . Elapsed : 0:00:52\n",
      "Batch 200 of 438 . Elapsed : 0:00:55\n",
      "Batch 210 of 438 . Elapsed : 0:00:57\n",
      "Batch 220 of 438 . Elapsed : 0:01:00\n",
      "Batch 230 of 438 . Elapsed : 0:01:03\n",
      "Batch 240 of 438 . Elapsed : 0:01:06\n",
      "Batch 250 of 438 . Elapsed : 0:01:09\n",
      "Batch 260 of 438 . Elapsed : 0:01:11\n",
      "Batch 270 of 438 . Elapsed : 0:01:14\n",
      "Batch 280 of 438 . Elapsed : 0:01:17\n",
      "Batch 290 of 438 . Elapsed : 0:01:20\n",
      "Batch 300 of 438 . Elapsed : 0:01:23\n",
      "Batch 310 of 438 . Elapsed : 0:01:26\n",
      "Batch 320 of 438 . Elapsed : 0:01:29\n",
      "Batch 330 of 438 . Elapsed : 0:01:32\n",
      "Batch 340 of 438 . Elapsed : 0:01:34\n",
      "Batch 350 of 438 . Elapsed : 0:01:37\n",
      "Batch 360 of 438 . Elapsed : 0:01:40\n",
      "Batch 370 of 438 . Elapsed : 0:01:43\n",
      "Batch 380 of 438 . Elapsed : 0:01:46\n",
      "Batch 390 of 438 . Elapsed : 0:01:49\n",
      "Batch 400 of 438 . Elapsed : 0:01:52\n",
      "Batch 410 of 438 . Elapsed : 0:01:55\n",
      "Batch 420 of 438 . Elapsed : 0:01:57\n",
      "Batch 430 of 438 . Elapsed : 0:02:00\n",
      "\n",
      " Average training Loss : 0.25\n",
      "Training epoch took : 0:02:03\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.94\n",
      "Validation took : 0:00:38\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:08\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:16\n",
      "Batch 70 of 438 . Elapsed : 0:00:19\n",
      "Batch 80 of 438 . Elapsed : 0:00:22\n",
      "Batch 90 of 438 . Elapsed : 0:00:24\n",
      "Batch 100 of 438 . Elapsed : 0:00:27\n",
      "Batch 110 of 438 . Elapsed : 0:00:30\n",
      "Batch 120 of 438 . Elapsed : 0:00:33\n",
      "Batch 130 of 438 . Elapsed : 0:00:35\n",
      "Batch 140 of 438 . Elapsed : 0:00:38\n",
      "Batch 150 of 438 . Elapsed : 0:00:41\n",
      "Batch 160 of 438 . Elapsed : 0:00:43\n",
      "Batch 170 of 438 . Elapsed : 0:00:46\n",
      "Batch 180 of 438 . Elapsed : 0:00:49\n",
      "Batch 190 of 438 . Elapsed : 0:00:51\n",
      "Batch 200 of 438 . Elapsed : 0:00:54\n",
      "Batch 210 of 438 . Elapsed : 0:00:57\n",
      "Batch 220 of 438 . Elapsed : 0:00:59\n",
      "Batch 230 of 438 . Elapsed : 0:01:02\n",
      "Batch 240 of 438 . Elapsed : 0:01:05\n",
      "Batch 250 of 438 . Elapsed : 0:01:08\n",
      "Batch 260 of 438 . Elapsed : 0:01:10\n",
      "Batch 270 of 438 . Elapsed : 0:01:13\n",
      "Batch 280 of 438 . Elapsed : 0:01:16\n",
      "Batch 290 of 438 . Elapsed : 0:01:18\n",
      "Batch 300 of 438 . Elapsed : 0:01:21\n",
      "Batch 310 of 438 . Elapsed : 0:01:24\n",
      "Batch 320 of 438 . Elapsed : 0:01:27\n",
      "Batch 330 of 438 . Elapsed : 0:01:30\n",
      "Batch 340 of 438 . Elapsed : 0:01:32\n",
      "Batch 350 of 438 . Elapsed : 0:01:35\n",
      "Batch 360 of 438 . Elapsed : 0:01:38\n",
      "Batch 370 of 438 . Elapsed : 0:01:41\n",
      "Batch 380 of 438 . Elapsed : 0:01:44\n",
      "Batch 390 of 438 . Elapsed : 0:01:46\n",
      "Batch 400 of 438 . Elapsed : 0:01:49\n",
      "Batch 410 of 438 . Elapsed : 0:01:52\n",
      "Batch 420 of 438 . Elapsed : 0:01:55\n",
      "Batch 430 of 438 . Elapsed : 0:01:58\n",
      "\n",
      " Average training Loss : 0.19\n",
      "Training epoch took : 0:02:00\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.95\n",
      "Validation took : 0:00:38\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:05\n",
      "Batch 30 of 438 . Elapsed : 0:00:08\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:16\n",
      "Batch 70 of 438 . Elapsed : 0:00:19\n",
      "Batch 80 of 438 . Elapsed : 0:00:22\n",
      "Batch 90 of 438 . Elapsed : 0:00:25\n",
      "Batch 100 of 438 . Elapsed : 0:00:28\n",
      "Batch 110 of 438 . Elapsed : 0:00:31\n",
      "Batch 120 of 438 . Elapsed : 0:00:34\n",
      "Batch 130 of 438 . Elapsed : 0:00:36\n",
      "Batch 140 of 438 . Elapsed : 0:00:39\n",
      "Batch 150 of 438 . Elapsed : 0:00:42\n",
      "Batch 160 of 438 . Elapsed : 0:00:45\n",
      "Batch 170 of 438 . Elapsed : 0:00:48\n",
      "Batch 180 of 438 . Elapsed : 0:00:51\n",
      "Batch 190 of 438 . Elapsed : 0:00:53\n",
      "Batch 200 of 438 . Elapsed : 0:00:56\n",
      "Batch 210 of 438 . Elapsed : 0:00:59\n",
      "Batch 220 of 438 . Elapsed : 0:01:02\n",
      "Batch 230 of 438 . Elapsed : 0:01:05\n",
      "Batch 240 of 438 . Elapsed : 0:01:07\n",
      "Batch 250 of 438 . Elapsed : 0:01:10\n",
      "Batch 260 of 438 . Elapsed : 0:01:13\n",
      "Batch 270 of 438 . Elapsed : 0:01:16\n",
      "Batch 280 of 438 . Elapsed : 0:01:19\n",
      "Batch 290 of 438 . Elapsed : 0:01:22\n",
      "Batch 300 of 438 . Elapsed : 0:01:25\n",
      "Batch 310 of 438 . Elapsed : 0:01:27\n",
      "Batch 320 of 438 . Elapsed : 0:01:30\n",
      "Batch 330 of 438 . Elapsed : 0:01:33\n",
      "Batch 340 of 438 . Elapsed : 0:01:36\n",
      "Batch 350 of 438 . Elapsed : 0:01:39\n",
      "Batch 360 of 438 . Elapsed : 0:01:42\n",
      "Batch 370 of 438 . Elapsed : 0:01:45\n",
      "Batch 380 of 438 . Elapsed : 0:01:48\n",
      "Batch 390 of 438 . Elapsed : 0:01:50\n",
      "Batch 400 of 438 . Elapsed : 0:01:53\n",
      "Batch 410 of 438 . Elapsed : 0:01:56\n",
      "Batch 420 of 438 . Elapsed : 0:01:59\n",
      "Batch 430 of 438 . Elapsed : 0:02:02\n",
      "\n",
      " Average training Loss : 0.15\n",
      "Training epoch took : 0:02:04\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.96\n",
      "Validation took : 0:00:37\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:08\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:17\n",
      "Batch 70 of 438 . Elapsed : 0:00:20\n",
      "Batch 80 of 438 . Elapsed : 0:00:22\n",
      "Batch 90 of 438 . Elapsed : 0:00:25\n",
      "Batch 100 of 438 . Elapsed : 0:00:28\n",
      "Batch 110 of 438 . Elapsed : 0:00:31\n",
      "Batch 120 of 438 . Elapsed : 0:00:34\n",
      "Batch 130 of 438 . Elapsed : 0:00:36\n",
      "Batch 140 of 438 . Elapsed : 0:00:39\n",
      "Batch 150 of 438 . Elapsed : 0:00:42\n",
      "Batch 160 of 438 . Elapsed : 0:00:45\n",
      "Batch 170 of 438 . Elapsed : 0:00:48\n",
      "Batch 180 of 438 . Elapsed : 0:00:51\n",
      "Batch 190 of 438 . Elapsed : 0:00:54\n",
      "Batch 200 of 438 . Elapsed : 0:00:57\n",
      "Batch 210 of 438 . Elapsed : 0:00:59\n",
      "Batch 220 of 438 . Elapsed : 0:01:02\n",
      "Batch 230 of 438 . Elapsed : 0:01:05\n",
      "Batch 240 of 438 . Elapsed : 0:01:08\n",
      "Batch 250 of 438 . Elapsed : 0:01:11\n",
      "Batch 260 of 438 . Elapsed : 0:01:14\n",
      "Batch 270 of 438 . Elapsed : 0:01:17\n",
      "Batch 280 of 438 . Elapsed : 0:01:19\n",
      "Batch 290 of 438 . Elapsed : 0:01:22\n",
      "Batch 300 of 438 . Elapsed : 0:01:25\n",
      "Batch 310 of 438 . Elapsed : 0:01:28\n",
      "Batch 320 of 438 . Elapsed : 0:01:31\n",
      "Batch 330 of 438 . Elapsed : 0:01:33\n",
      "Batch 340 of 438 . Elapsed : 0:01:36\n",
      "Batch 350 of 438 . Elapsed : 0:01:39\n",
      "Batch 360 of 438 . Elapsed : 0:01:42\n",
      "Batch 370 of 438 . Elapsed : 0:01:44\n",
      "Batch 380 of 438 . Elapsed : 0:01:47\n",
      "Batch 390 of 438 . Elapsed : 0:01:50\n",
      "Batch 400 of 438 . Elapsed : 0:01:53\n",
      "Batch 410 of 438 . Elapsed : 0:01:55\n",
      "Batch 420 of 438 . Elapsed : 0:01:58\n",
      "Batch 430 of 438 . Elapsed : 0:02:01\n",
      "\n",
      " Average training Loss : 0.13\n",
      "Training epoch took : 0:02:03\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.96\n",
      "Validation took : 0:00:38\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:09\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:17\n",
      "Batch 70 of 438 . Elapsed : 0:00:20\n",
      "Batch 80 of 438 . Elapsed : 0:00:23\n",
      "Batch 90 of 438 . Elapsed : 0:00:25\n",
      "Batch 100 of 438 . Elapsed : 0:00:28\n",
      "Batch 110 of 438 . Elapsed : 0:00:31\n",
      "Batch 120 of 438 . Elapsed : 0:00:33\n",
      "Batch 130 of 438 . Elapsed : 0:00:36\n",
      "Batch 140 of 438 . Elapsed : 0:00:39\n",
      "Batch 150 of 438 . Elapsed : 0:00:42\n",
      "Batch 160 of 438 . Elapsed : 0:00:44\n",
      "Batch 170 of 438 . Elapsed : 0:00:47\n",
      "Batch 180 of 438 . Elapsed : 0:00:50\n",
      "Batch 190 of 438 . Elapsed : 0:00:52\n",
      "Batch 200 of 438 . Elapsed : 0:00:55\n",
      "Batch 210 of 438 . Elapsed : 0:00:58\n",
      "Batch 220 of 438 . Elapsed : 0:01:00\n",
      "Batch 230 of 438 . Elapsed : 0:01:03\n",
      "Batch 240 of 438 . Elapsed : 0:01:06\n",
      "Batch 250 of 438 . Elapsed : 0:01:08\n",
      "Batch 260 of 438 . Elapsed : 0:01:11\n",
      "Batch 270 of 438 . Elapsed : 0:01:14\n",
      "Batch 280 of 438 . Elapsed : 0:01:17\n",
      "Batch 290 of 438 . Elapsed : 0:01:19\n",
      "Batch 300 of 438 . Elapsed : 0:01:22\n",
      "Batch 310 of 438 . Elapsed : 0:01:25\n",
      "Batch 320 of 438 . Elapsed : 0:01:27\n",
      "Batch 330 of 438 . Elapsed : 0:01:30\n",
      "Batch 340 of 438 . Elapsed : 0:01:33\n",
      "Batch 350 of 438 . Elapsed : 0:01:35\n",
      "Batch 360 of 438 . Elapsed : 0:01:38\n",
      "Batch 370 of 438 . Elapsed : 0:01:41\n",
      "Batch 380 of 438 . Elapsed : 0:01:43\n",
      "Batch 390 of 438 . Elapsed : 0:01:46\n",
      "Batch 400 of 438 . Elapsed : 0:01:49\n",
      "Batch 410 of 438 . Elapsed : 0:01:51\n",
      "Batch 420 of 438 . Elapsed : 0:01:54\n",
      "Batch 430 of 438 . Elapsed : 0:01:57\n",
      "\n",
      " Average training Loss : 0.12\n",
      "Training epoch took : 0:01:59\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.96\n",
      "Validation took : 0:00:36\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:08\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:16\n",
      "Batch 70 of 438 . Elapsed : 0:00:19\n",
      "Batch 80 of 438 . Elapsed : 0:00:22\n",
      "Batch 90 of 438 . Elapsed : 0:00:24\n",
      "Batch 100 of 438 . Elapsed : 0:00:27\n",
      "Batch 110 of 438 . Elapsed : 0:00:30\n",
      "Batch 120 of 438 . Elapsed : 0:00:32\n",
      "Batch 130 of 438 . Elapsed : 0:00:35\n",
      "Batch 140 of 438 . Elapsed : 0:00:38\n",
      "Batch 150 of 438 . Elapsed : 0:00:41\n",
      "Batch 160 of 438 . Elapsed : 0:00:43\n",
      "Batch 170 of 438 . Elapsed : 0:00:46\n",
      "Batch 180 of 438 . Elapsed : 0:00:49\n",
      "Batch 190 of 438 . Elapsed : 0:00:51\n",
      "Batch 200 of 438 . Elapsed : 0:00:54\n",
      "Batch 210 of 438 . Elapsed : 0:00:57\n",
      "Batch 220 of 438 . Elapsed : 0:00:59\n",
      "Batch 230 of 438 . Elapsed : 0:01:02\n",
      "Batch 240 of 438 . Elapsed : 0:01:05\n",
      "Batch 250 of 438 . Elapsed : 0:01:07\n",
      "Batch 260 of 438 . Elapsed : 0:01:10\n",
      "Batch 270 of 438 . Elapsed : 0:01:13\n",
      "Batch 280 of 438 . Elapsed : 0:01:16\n",
      "Batch 290 of 438 . Elapsed : 0:01:18\n",
      "Batch 300 of 438 . Elapsed : 0:01:21\n",
      "Batch 310 of 438 . Elapsed : 0:01:24\n",
      "Batch 320 of 438 . Elapsed : 0:01:27\n",
      "Batch 330 of 438 . Elapsed : 0:01:29\n",
      "Batch 340 of 438 . Elapsed : 0:01:32\n",
      "Batch 350 of 438 . Elapsed : 0:01:35\n",
      "Batch 360 of 438 . Elapsed : 0:01:38\n",
      "Batch 370 of 438 . Elapsed : 0:01:41\n",
      "Batch 380 of 438 . Elapsed : 0:01:43\n",
      "Batch 390 of 438 . Elapsed : 0:01:46\n",
      "Batch 400 of 438 . Elapsed : 0:01:49\n",
      "Batch 410 of 438 . Elapsed : 0:01:52\n",
      "Batch 420 of 438 . Elapsed : 0:01:55\n",
      "Batch 430 of 438 . Elapsed : 0:01:57\n",
      "\n",
      " Average training Loss : 0.10\n",
      "Training epoch took : 0:02:00\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.97\n",
      "Validation took : 0:00:38\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:09\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:17\n",
      "Batch 70 of 438 . Elapsed : 0:00:20\n",
      "Batch 80 of 438 . Elapsed : 0:00:23\n",
      "Batch 90 of 438 . Elapsed : 0:00:26\n",
      "Batch 100 of 438 . Elapsed : 0:00:28\n",
      "Batch 110 of 438 . Elapsed : 0:00:31\n",
      "Batch 120 of 438 . Elapsed : 0:00:34\n",
      "Batch 130 of 438 . Elapsed : 0:00:37\n",
      "Batch 140 of 438 . Elapsed : 0:00:40\n",
      "Batch 150 of 438 . Elapsed : 0:00:43\n",
      "Batch 160 of 438 . Elapsed : 0:00:45\n",
      "Batch 170 of 438 . Elapsed : 0:00:48\n",
      "Batch 180 of 438 . Elapsed : 0:00:51\n",
      "Batch 190 of 438 . Elapsed : 0:00:54\n",
      "Batch 200 of 438 . Elapsed : 0:00:57\n",
      "Batch 210 of 438 . Elapsed : 0:00:59\n",
      "Batch 220 of 438 . Elapsed : 0:01:02\n",
      "Batch 230 of 438 . Elapsed : 0:01:05\n",
      "Batch 240 of 438 . Elapsed : 0:01:07\n",
      "Batch 250 of 438 . Elapsed : 0:01:10\n",
      "Batch 260 of 438 . Elapsed : 0:01:13\n",
      "Batch 270 of 438 . Elapsed : 0:01:15\n",
      "Batch 280 of 438 . Elapsed : 0:01:18\n",
      "Batch 290 of 438 . Elapsed : 0:01:21\n",
      "Batch 300 of 438 . Elapsed : 0:01:23\n",
      "Batch 310 of 438 . Elapsed : 0:01:26\n",
      "Batch 320 of 438 . Elapsed : 0:01:29\n",
      "Batch 330 of 438 . Elapsed : 0:01:32\n",
      "Batch 340 of 438 . Elapsed : 0:01:34\n",
      "Batch 350 of 438 . Elapsed : 0:01:37\n",
      "Batch 360 of 438 . Elapsed : 0:01:40\n",
      "Batch 370 of 438 . Elapsed : 0:01:42\n",
      "Batch 380 of 438 . Elapsed : 0:01:45\n",
      "Batch 390 of 438 . Elapsed : 0:01:48\n",
      "Batch 400 of 438 . Elapsed : 0:01:50\n",
      "Batch 410 of 438 . Elapsed : 0:01:53\n",
      "Batch 420 of 438 . Elapsed : 0:01:56\n",
      "Batch 430 of 438 . Elapsed : 0:01:59\n",
      "\n",
      " Average training Loss : 0.09\n",
      "Training epoch took : 0:02:01\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.97\n",
      "Validation took : 0:00:37\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:05\n",
      "Batch 30 of 438 . Elapsed : 0:00:08\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:13\n",
      "Batch 60 of 438 . Elapsed : 0:00:16\n",
      "Batch 70 of 438 . Elapsed : 0:00:18\n",
      "Batch 80 of 438 . Elapsed : 0:00:21\n",
      "Batch 90 of 438 . Elapsed : 0:00:24\n",
      "Batch 100 of 438 . Elapsed : 0:00:26\n",
      "Batch 110 of 438 . Elapsed : 0:00:29\n",
      "Batch 120 of 438 . Elapsed : 0:00:32\n",
      "Batch 130 of 438 . Elapsed : 0:00:35\n",
      "Batch 140 of 438 . Elapsed : 0:00:37\n",
      "Batch 150 of 438 . Elapsed : 0:00:40\n",
      "Batch 160 of 438 . Elapsed : 0:00:43\n",
      "Batch 170 of 438 . Elapsed : 0:00:46\n",
      "Batch 180 of 438 . Elapsed : 0:00:48\n",
      "Batch 190 of 438 . Elapsed : 0:00:51\n",
      "Batch 200 of 438 . Elapsed : 0:00:54\n",
      "Batch 210 of 438 . Elapsed : 0:00:57\n",
      "Batch 220 of 438 . Elapsed : 0:01:00\n",
      "Batch 230 of 438 . Elapsed : 0:01:02\n",
      "Batch 240 of 438 . Elapsed : 0:01:05\n",
      "Batch 250 of 438 . Elapsed : 0:01:08\n",
      "Batch 260 of 438 . Elapsed : 0:01:11\n",
      "Batch 270 of 438 . Elapsed : 0:01:14\n",
      "Batch 280 of 438 . Elapsed : 0:01:16\n",
      "Batch 290 of 438 . Elapsed : 0:01:19\n",
      "Batch 300 of 438 . Elapsed : 0:01:22\n",
      "Batch 310 of 438 . Elapsed : 0:01:25\n",
      "Batch 320 of 438 . Elapsed : 0:01:28\n",
      "Batch 330 of 438 . Elapsed : 0:01:30\n",
      "Batch 340 of 438 . Elapsed : 0:01:33\n",
      "Batch 350 of 438 . Elapsed : 0:01:36\n",
      "Batch 360 of 438 . Elapsed : 0:01:39\n",
      "Batch 370 of 438 . Elapsed : 0:01:41\n",
      "Batch 380 of 438 . Elapsed : 0:01:44\n",
      "Batch 390 of 438 . Elapsed : 0:01:47\n",
      "Batch 400 of 438 . Elapsed : 0:01:50\n",
      "Batch 410 of 438 . Elapsed : 0:01:53\n",
      "Batch 420 of 438 . Elapsed : 0:01:55\n",
      "Batch 430 of 438 . Elapsed : 0:01:58\n",
      "\n",
      " Average training Loss : 0.09\n",
      "Training epoch took : 0:02:00\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.97\n",
      "Validation took : 0:00:38\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n",
      "Batch 10 of 438 . Elapsed : 0:00:03\n",
      "Batch 20 of 438 . Elapsed : 0:00:06\n",
      "Batch 30 of 438 . Elapsed : 0:00:08\n",
      "Batch 40 of 438 . Elapsed : 0:00:11\n",
      "Batch 50 of 438 . Elapsed : 0:00:14\n",
      "Batch 60 of 438 . Elapsed : 0:00:17\n",
      "Batch 70 of 438 . Elapsed : 0:00:20\n",
      "Batch 80 of 438 . Elapsed : 0:00:22\n",
      "Batch 90 of 438 . Elapsed : 0:00:25\n",
      "Batch 100 of 438 . Elapsed : 0:00:28\n",
      "Batch 110 of 438 . Elapsed : 0:00:31\n",
      "Batch 120 of 438 . Elapsed : 0:00:33\n",
      "Batch 130 of 438 . Elapsed : 0:00:36\n",
      "Batch 140 of 438 . Elapsed : 0:00:39\n",
      "Batch 150 of 438 . Elapsed : 0:00:42\n",
      "Batch 160 of 438 . Elapsed : 0:00:45\n",
      "Batch 170 of 438 . Elapsed : 0:00:47\n",
      "Batch 180 of 438 . Elapsed : 0:00:50\n",
      "Batch 190 of 438 . Elapsed : 0:00:53\n",
      "Batch 200 of 438 . Elapsed : 0:00:56\n",
      "Batch 210 of 438 . Elapsed : 0:00:58\n",
      "Batch 220 of 438 . Elapsed : 0:01:01\n",
      "Batch 230 of 438 . Elapsed : 0:01:04\n",
      "Batch 240 of 438 . Elapsed : 0:01:06\n",
      "Batch 250 of 438 . Elapsed : 0:01:09\n",
      "Batch 260 of 438 . Elapsed : 0:01:12\n",
      "Batch 270 of 438 . Elapsed : 0:01:15\n",
      "Batch 280 of 438 . Elapsed : 0:01:17\n",
      "Batch 290 of 438 . Elapsed : 0:01:20\n",
      "Batch 300 of 438 . Elapsed : 0:01:23\n",
      "Batch 310 of 438 . Elapsed : 0:01:26\n",
      "Batch 320 of 438 . Elapsed : 0:01:29\n",
      "Batch 330 of 438 . Elapsed : 0:01:31\n",
      "Batch 340 of 438 . Elapsed : 0:01:34\n",
      "Batch 350 of 438 . Elapsed : 0:01:37\n",
      "Batch 360 of 438 . Elapsed : 0:01:40\n",
      "Batch 370 of 438 . Elapsed : 0:01:43\n",
      "Batch 380 of 438 . Elapsed : 0:01:46\n",
      "Batch 390 of 438 . Elapsed : 0:01:48\n",
      "Batch 400 of 438 . Elapsed : 0:01:51\n",
      "Batch 410 of 438 . Elapsed : 0:01:54\n",
      "Batch 420 of 438 . Elapsed : 0:01:56\n",
      "Batch 430 of 438 . Elapsed : 0:01:59\n",
      "\n",
      " Average training Loss : 0.08\n",
      "Training epoch took : 0:02:01\n",
      "\n",
      "Validation...\n",
      "Accuracy : 0.97\n",
      "Validation took : 0:00:35\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "seed_val = 24242\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0,epochs):\n",
    "    print(\"\")\n",
    "    print(f\"======== Epoch {epoch_i+1} / {epochs} ========\")\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        if step%10==0 and not step==0:\n",
    "            elapsed = format_time(time.time()-t0)\n",
    "            print(f\"Batch {step} of {len(train_dataloader)} . Elapsed : {elapsed}\")\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].type(torch.LongTensor).to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)  \n",
    "    print(\"\")\n",
    "    print(f\" Average training Loss : {avg_train_loss:.2f}\")\n",
    "    print(f\"Training epoch took : {format_time(time.time()-t0)}\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Validation...\")\n",
    "    t0=time.time()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(x.to(device) for x in batch)\n",
    "        b_input_ids , b_input_mask , b_labels = batch\n",
    "        b_labels = b_labels.type(torch.LongTensor).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits,label_ids)\n",
    "        eval_accuracy+=tmp_eval_accuracy\n",
    "        nb_eval_steps+=1\n",
    "    print(f\"Accuracy : {eval_accuracy/nb_eval_steps:.2f}\")\n",
    "    print(f\"Validation took : {format_time(time.time()-t0)}\")\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "output_dir = \"./model_save/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(f\"Saving to {output_dir}\")\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model ",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
